{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Immigration Analysis\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "This project intends to prepare an analytical database for the study of immigration into the United States based on the I94 data. This incorporates the design of an ETL process which includes data modeling (i.e. a database schema) and data cleansing. The main focus of the analytical database will lie on:\n",
    "\n",
    "* Mode of entrance (vehicle and visa type)\n",
    "* Destination and origin of travel\n",
    "* Date of entrance and duration of stay in the country\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "The project consists of the following steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Do all imports and installs here\n",
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "# We will be using PySpark, as will be justified later:\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, expr, pandas_udf, year, month, dayofmonth\n",
    "from pyspark.sql.types import StructType, StructField, DoubleType, StringType, IntegerType, DateType, TimestampType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "*Explain what you plan to do in the project in more detail. What data do you use? What is your end solution look like? What tools did you use? etc>*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "We intend to make a database model, i.e. design a database relationship model, and an ETL process. \n",
    "Our main tools will be `Pandas` for initial data exploration, and -- for the heavy weight lifting -- Spark (in particular `PySpark`). For data storage, we will use `Parquet` since this data format allows for fast and efficient querying."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Describe and Gather Data \n",
    "*Describe the data sets you're using. Where did it come from? What type of information is included?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "We will use US immigration data from the year 2016 which has been provided by the US National Tourism and Trade office through its website. This dataset includes data such as the place of immigration (border), the visa type, the date of arrival, the duration of stay, and many more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## US immigration data from the year 2016 - Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>cicid</th>\n",
       "      <th>i94yr</th>\n",
       "      <th>i94mon</th>\n",
       "      <th>i94cit</th>\n",
       "      <th>i94res</th>\n",
       "      <th>i94port</th>\n",
       "      <th>arrdate</th>\n",
       "      <th>i94mode</th>\n",
       "      <th>i94addr</th>\n",
       "      <th>...</th>\n",
       "      <th>entdepu</th>\n",
       "      <th>matflag</th>\n",
       "      <th>biryear</th>\n",
       "      <th>dtaddto</th>\n",
       "      <th>gender</th>\n",
       "      <th>insnum</th>\n",
       "      <th>airline</th>\n",
       "      <th>admnum</th>\n",
       "      <th>fltno</th>\n",
       "      <th>visatype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2027561</td>\n",
       "      <td>4084316.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>209.0</td>\n",
       "      <td>209.0</td>\n",
       "      <td>HHW</td>\n",
       "      <td>20566.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>HI</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1955.0</td>\n",
       "      <td>07202016</td>\n",
       "      <td>F</td>\n",
       "      <td>NaN</td>\n",
       "      <td>JL</td>\n",
       "      <td>5.658267e+10</td>\n",
       "      <td>00782</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2171295</td>\n",
       "      <td>4422636.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>582.0</td>\n",
       "      <td>582.0</td>\n",
       "      <td>MCA</td>\n",
       "      <td>20567.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>TX</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1990.0</td>\n",
       "      <td>10222016</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>*GA</td>\n",
       "      <td>9.436200e+10</td>\n",
       "      <td>XBLNG</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>589494</td>\n",
       "      <td>1195600.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>148.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>OGG</td>\n",
       "      <td>20551.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>FL</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1940.0</td>\n",
       "      <td>07052016</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LH</td>\n",
       "      <td>5.578047e+10</td>\n",
       "      <td>00464</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2631158</td>\n",
       "      <td>5291768.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>297.0</td>\n",
       "      <td>297.0</td>\n",
       "      <td>LOS</td>\n",
       "      <td>20572.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>CA</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1991.0</td>\n",
       "      <td>10272016</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>QR</td>\n",
       "      <td>9.478970e+10</td>\n",
       "      <td>00739</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3032257</td>\n",
       "      <td>985523.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>CHM</td>\n",
       "      <td>20550.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NY</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1997.0</td>\n",
       "      <td>07042016</td>\n",
       "      <td>F</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.232257e+10</td>\n",
       "      <td>LAND</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>721257</td>\n",
       "      <td>1481650.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>577.0</td>\n",
       "      <td>577.0</td>\n",
       "      <td>ATL</td>\n",
       "      <td>20552.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>GA</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1965.0</td>\n",
       "      <td>10072016</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>DL</td>\n",
       "      <td>7.368526e+08</td>\n",
       "      <td>910</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1072780</td>\n",
       "      <td>2197173.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>SFR</td>\n",
       "      <td>20556.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>CA</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1968.0</td>\n",
       "      <td>10112016</td>\n",
       "      <td>F</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CX</td>\n",
       "      <td>7.863122e+08</td>\n",
       "      <td>870</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>112205</td>\n",
       "      <td>232708.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>113.0</td>\n",
       "      <td>135.0</td>\n",
       "      <td>NYC</td>\n",
       "      <td>20546.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NY</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1983.0</td>\n",
       "      <td>06302016</td>\n",
       "      <td>F</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BA</td>\n",
       "      <td>5.547449e+10</td>\n",
       "      <td>00117</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2577162</td>\n",
       "      <td>5227851.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>131.0</td>\n",
       "      <td>131.0</td>\n",
       "      <td>CHI</td>\n",
       "      <td>20572.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>IL</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1977.0</td>\n",
       "      <td>07262016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LX</td>\n",
       "      <td>5.941342e+10</td>\n",
       "      <td>00008</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10930</td>\n",
       "      <td>13213.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>LOS</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>CA</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1981.0</td>\n",
       "      <td>06292016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AA</td>\n",
       "      <td>5.544979e+10</td>\n",
       "      <td>00109</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0      cicid   i94yr  i94mon  i94cit  i94res i94port  arrdate  \\\n",
       "0     2027561  4084316.0  2016.0     4.0   209.0   209.0     HHW  20566.0   \n",
       "1     2171295  4422636.0  2016.0     4.0   582.0   582.0     MCA  20567.0   \n",
       "2      589494  1195600.0  2016.0     4.0   148.0   112.0     OGG  20551.0   \n",
       "3     2631158  5291768.0  2016.0     4.0   297.0   297.0     LOS  20572.0   \n",
       "4     3032257   985523.0  2016.0     4.0   111.0   111.0     CHM  20550.0   \n",
       "5      721257  1481650.0  2016.0     4.0   577.0   577.0     ATL  20552.0   \n",
       "6     1072780  2197173.0  2016.0     4.0   245.0   245.0     SFR  20556.0   \n",
       "7      112205   232708.0  2016.0     4.0   113.0   135.0     NYC  20546.0   \n",
       "8     2577162  5227851.0  2016.0     4.0   131.0   131.0     CHI  20572.0   \n",
       "9       10930    13213.0  2016.0     4.0   116.0   116.0     LOS  20545.0   \n",
       "\n",
       "   i94mode i94addr    ...     entdepu  matflag  biryear   dtaddto  gender  \\\n",
       "0      1.0      HI    ...         NaN        M   1955.0  07202016       F   \n",
       "1      1.0      TX    ...         NaN        M   1990.0  10222016       M   \n",
       "2      1.0      FL    ...         NaN        M   1940.0  07052016       M   \n",
       "3      1.0      CA    ...         NaN        M   1991.0  10272016       M   \n",
       "4      3.0      NY    ...         NaN        M   1997.0  07042016       F   \n",
       "5      1.0      GA    ...         NaN        M   1965.0  10072016       M   \n",
       "6      1.0      CA    ...         NaN        M   1968.0  10112016       F   \n",
       "7      1.0      NY    ...         NaN        M   1983.0  06302016       F   \n",
       "8      1.0      IL    ...         NaN        M   1977.0  07262016     NaN   \n",
       "9      1.0      CA    ...         NaN        M   1981.0  06292016     NaN   \n",
       "\n",
       "  insnum airline        admnum  fltno  visatype  \n",
       "0    NaN      JL  5.658267e+10  00782        WT  \n",
       "1    NaN     *GA  9.436200e+10  XBLNG        B2  \n",
       "2    NaN      LH  5.578047e+10  00464        WT  \n",
       "3    NaN      QR  9.478970e+10  00739        B2  \n",
       "4    NaN     NaN  4.232257e+10   LAND        WT  \n",
       "5    NaN      DL  7.368526e+08    910        B2  \n",
       "6    NaN      CX  7.863122e+08    870        B2  \n",
       "7    NaN      BA  5.547449e+10  00117        WT  \n",
       "8    NaN      LX  5.941342e+10  00008        WT  \n",
       "9    NaN      AA  5.544979e+10  00109        WT  \n",
       "\n",
       "[10 rows x 29 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pandas = pd.read_csv('immigration_data_sample.csv')\n",
    "df_pandas.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Airport Codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ident</th>\n",
       "      <th>type</th>\n",
       "      <th>name</th>\n",
       "      <th>elevation_ft</th>\n",
       "      <th>continent</th>\n",
       "      <th>iso_country</th>\n",
       "      <th>iso_region</th>\n",
       "      <th>municipality</th>\n",
       "      <th>gps_code</th>\n",
       "      <th>iata_code</th>\n",
       "      <th>local_code</th>\n",
       "      <th>coordinates</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00A</td>\n",
       "      <td>heliport</td>\n",
       "      <td>Total Rf Heliport</td>\n",
       "      <td>11.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-PA</td>\n",
       "      <td>Bensalem</td>\n",
       "      <td>00A</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00A</td>\n",
       "      <td>-74.93360137939453, 40.07080078125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00AA</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Aero B Ranch Airport</td>\n",
       "      <td>3435.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-KS</td>\n",
       "      <td>Leoti</td>\n",
       "      <td>00AA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00AA</td>\n",
       "      <td>-101.473911, 38.704022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00AK</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Lowell Field</td>\n",
       "      <td>450.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AK</td>\n",
       "      <td>Anchor Point</td>\n",
       "      <td>00AK</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00AK</td>\n",
       "      <td>-151.695999146, 59.94919968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00AL</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Epps Airpark</td>\n",
       "      <td>820.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AL</td>\n",
       "      <td>Harvest</td>\n",
       "      <td>00AL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00AL</td>\n",
       "      <td>-86.77030181884766, 34.86479949951172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00AR</td>\n",
       "      <td>closed</td>\n",
       "      <td>Newport Hospital &amp; Clinic Heliport</td>\n",
       "      <td>237.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AR</td>\n",
       "      <td>Newport</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-91.254898, 35.6087</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ident           type                                name  elevation_ft  \\\n",
       "0   00A       heliport                   Total Rf Heliport          11.0   \n",
       "1  00AA  small_airport                Aero B Ranch Airport        3435.0   \n",
       "2  00AK  small_airport                        Lowell Field         450.0   \n",
       "3  00AL  small_airport                        Epps Airpark         820.0   \n",
       "4  00AR         closed  Newport Hospital & Clinic Heliport         237.0   \n",
       "\n",
       "  continent iso_country iso_region  municipality gps_code iata_code  \\\n",
       "0       NaN          US      US-PA      Bensalem      00A       NaN   \n",
       "1       NaN          US      US-KS         Leoti     00AA       NaN   \n",
       "2       NaN          US      US-AK  Anchor Point     00AK       NaN   \n",
       "3       NaN          US      US-AL       Harvest     00AL       NaN   \n",
       "4       NaN          US      US-AR       Newport      NaN       NaN   \n",
       "\n",
       "  local_code                            coordinates  \n",
       "0        00A     -74.93360137939453, 40.07080078125  \n",
       "1       00AA                 -101.473911, 38.704022  \n",
       "2       00AK            -151.695999146, 59.94919968  \n",
       "3       00AL  -86.77030181884766, 34.86479949951172  \n",
       "4        NaN                    -91.254898, 35.6087  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pandas = pd.read_csv('airport-codes_csv.csv')\n",
    "df_pandas.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Demographics of US-Cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Median Age</th>\n",
       "      <th>Male Population</th>\n",
       "      <th>Female Population</th>\n",
       "      <th>Total Population</th>\n",
       "      <th>Number of Veterans</th>\n",
       "      <th>Foreign-born</th>\n",
       "      <th>Average Household Size</th>\n",
       "      <th>State Code</th>\n",
       "      <th>Race</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Silver Spring</td>\n",
       "      <td>Maryland</td>\n",
       "      <td>33.8</td>\n",
       "      <td>40601.0</td>\n",
       "      <td>41862.0</td>\n",
       "      <td>82463</td>\n",
       "      <td>1562.0</td>\n",
       "      <td>30908.0</td>\n",
       "      <td>2.60</td>\n",
       "      <td>MD</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "      <td>25924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Quincy</td>\n",
       "      <td>Massachusetts</td>\n",
       "      <td>41.0</td>\n",
       "      <td>44129.0</td>\n",
       "      <td>49500.0</td>\n",
       "      <td>93629</td>\n",
       "      <td>4147.0</td>\n",
       "      <td>32935.0</td>\n",
       "      <td>2.39</td>\n",
       "      <td>MA</td>\n",
       "      <td>White</td>\n",
       "      <td>58723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hoover</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>38.5</td>\n",
       "      <td>38040.0</td>\n",
       "      <td>46799.0</td>\n",
       "      <td>84839</td>\n",
       "      <td>4819.0</td>\n",
       "      <td>8229.0</td>\n",
       "      <td>2.58</td>\n",
       "      <td>AL</td>\n",
       "      <td>Asian</td>\n",
       "      <td>4759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rancho Cucamonga</td>\n",
       "      <td>California</td>\n",
       "      <td>34.5</td>\n",
       "      <td>88127.0</td>\n",
       "      <td>87105.0</td>\n",
       "      <td>175232</td>\n",
       "      <td>5821.0</td>\n",
       "      <td>33878.0</td>\n",
       "      <td>3.18</td>\n",
       "      <td>CA</td>\n",
       "      <td>Black or African-American</td>\n",
       "      <td>24437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Newark</td>\n",
       "      <td>New Jersey</td>\n",
       "      <td>34.6</td>\n",
       "      <td>138040.0</td>\n",
       "      <td>143873.0</td>\n",
       "      <td>281913</td>\n",
       "      <td>5829.0</td>\n",
       "      <td>86253.0</td>\n",
       "      <td>2.73</td>\n",
       "      <td>NJ</td>\n",
       "      <td>White</td>\n",
       "      <td>76402</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               City          State  Median Age  Male Population  \\\n",
       "0     Silver Spring       Maryland        33.8          40601.0   \n",
       "1            Quincy  Massachusetts        41.0          44129.0   \n",
       "2            Hoover        Alabama        38.5          38040.0   \n",
       "3  Rancho Cucamonga     California        34.5          88127.0   \n",
       "4            Newark     New Jersey        34.6         138040.0   \n",
       "\n",
       "   Female Population  Total Population  Number of Veterans  Foreign-born  \\\n",
       "0            41862.0             82463              1562.0       30908.0   \n",
       "1            49500.0             93629              4147.0       32935.0   \n",
       "2            46799.0             84839              4819.0        8229.0   \n",
       "3            87105.0            175232              5821.0       33878.0   \n",
       "4           143873.0            281913              5829.0       86253.0   \n",
       "\n",
       "   Average Household Size State Code                       Race  Count  \n",
       "0                    2.60         MD         Hispanic or Latino  25924  \n",
       "1                    2.39         MA                      White  58723  \n",
       "2                    2.58         AL                      Asian   4759  \n",
       "3                    3.18         CA  Black or African-American  24437  \n",
       "4                    2.73         NJ                      White  76402  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Strictly speaking, semicolon-separated data:\n",
    "df_pandas = pd.read_csv('us-cities-demographics.csv', sep=';')\n",
    "df_pandas.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Temperature Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dt</th>\n",
       "      <th>AverageTemperature</th>\n",
       "      <th>AverageTemperatureUncertainty</th>\n",
       "      <th>City</th>\n",
       "      <th>Country</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1743-11-01</td>\n",
       "      <td>6.068</td>\n",
       "      <td>1.737</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1743-12-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1744-01-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1744-02-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1744-03-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           dt  AverageTemperature  AverageTemperatureUncertainty   City  \\\n",
       "0  1743-11-01               6.068                          1.737  Århus   \n",
       "1  1743-12-01                 NaN                            NaN  Århus   \n",
       "2  1744-01-01                 NaN                            NaN  Århus   \n",
       "3  1744-02-01                 NaN                            NaN  Århus   \n",
       "4  1744-03-01                 NaN                            NaN  Århus   \n",
       "\n",
       "   Country Latitude Longitude  \n",
       "0  Denmark   57.05N    10.33E  \n",
       "1  Denmark   57.05N    10.33E  \n",
       "2  Denmark   57.05N    10.33E  \n",
       "3  Denmark   57.05N    10.33E  \n",
       "4  Denmark   57.05N    10.33E  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fname = '../../data2/GlobalLandTemperaturesByCity.csv'\n",
    "df_pandas = pd.read_csv(fname)\n",
    "df_pandas.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Utilizing Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Since this is only a sample, we will try out Pandas with the monthly data of the whole dataset. As per the instructions, the raw immigration data is stored in the directory `../../data/18-83510-I94-Data-2016/` and is partitioned into monthly datasets as indicated by three-letter month codes, e.g. `i94_jan16_sub.sas7bdat`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "fname = '../../data/18-83510-I94-Data-2016/i94_jan16_sub.sas7bdat'\n",
    "df_pandas = pd.read_sas(fname, 'sas7bdat', encoding=\"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "As expected, the processing takes too long, and we will have to utilize a genuine big data toolbox: *Spark*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.\\\n",
    "    config(\"spark.jars.repositories\", \"https://repos.spark-packages.org/\").\\\n",
    "    config(\"spark.jars.packages\", \"saurfang:spark-sas7bdat:2.0.0-s_2.11\").\\\n",
    "    enableHiveSupport().getOrCreate()\n",
    "\n",
    "df = spark.read.format('com.github.saurfang.sas.spark').load('../../data/18-83510-I94-Data-2016/i94_jan16_sub.sas7bdat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "In order to work more efficiently with higher i/o-performance, we will convert the raw immigration data into **Parquet** format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Work with parquet\n",
    "df.write.parquet(\"data_parquet/exploration\")\n",
    "df = spark.read.parquet(\"data_parquet/exploration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df = spark.read.parquet(\"data_parquet/exploration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "For reference: Number of rows in the raw dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2847924"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+------------+-----+--------+\n",
      "|cicid| i94yr|i94mon|i94cit|i94res|i94port|arrdate|i94mode|i94addr|depdate|i94bir|i94visa|count|dtadfile|visapost|occup|entdepa|entdepd|entdepu|matflag|biryear| dtaddto|gender|insnum|airline|      admnum|fltno|visatype|\n",
      "+-----+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+------------+-----+--------+\n",
      "|  7.0|2016.0|   1.0| 101.0| 101.0|    BOS|20465.0|    1.0|     MA|   null|  20.0|    3.0|  1.0|    null|    null| null|      T|   null|   null|   null| 1996.0|     D/S|     M|  null|     LH|3.46608285E8|  424|      F1|\n",
      "|  8.0|2016.0|   1.0| 101.0| 101.0|    BOS|20465.0|    1.0|     MA|   null|  20.0|    3.0|  1.0|    null|    null| null|      T|   null|   null|   null| 1996.0|     D/S|     M|  null|     LH|3.46627585E8|  424|      F1|\n",
      "|  9.0|2016.0|   1.0| 101.0| 101.0|    BOS|20469.0|    1.0|     CT|20480.0|  17.0|    2.0|  1.0|    null|    null| null|      T|      N|   null|      M| 1999.0|07152016|     F|  null|     AF|3.81092385E8|  338|      B2|\n",
      "| 10.0|2016.0|   1.0| 101.0| 101.0|    BOS|20469.0|    1.0|     CT|20499.0|  45.0|    2.0|  1.0|    null|    null| null|      T|      N|   null|      M| 1971.0|07152016|     F|  null|     AF|3.81087885E8|  338|      B2|\n",
      "| 11.0|2016.0|   1.0| 101.0| 101.0|    BOS|20469.0|    1.0|     CT|20499.0|  12.0|    2.0|  1.0|    null|    null| null|      T|      N|   null|      M| 2004.0|07152016|     M|  null|     AF|3.81078685E8|  338|      B2|\n",
      "+-----+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+------------+-----+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "*Identify data quality issues, like missing values, duplicate data, etc.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.printSchema of DataFrame[cicid: double, i94yr: double, i94mon: double, i94cit: double, i94res: double, i94port: string, arrdate: double, i94mode: double, i94addr: string, depdate: double, i94bir: double, i94visa: double, count: double, dtadfile: string, visapost: string, occup: string, entdepa: string, entdepd: string, entdepu: string, matflag: string, biryear: double, dtaddto: string, gender: string, insnum: string, airline: string, admnum: double, fltno: string, visatype: string]>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.printSchema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Cleaning Steps\n",
    "*Document steps necessary to clean the data*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "We are looking for the following data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df = df.filter(\"i94cit == i94res and i94addr != '' and visatype != '' and arrdate is not null and depdate is not null\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1786071"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[cicid: double, i94yr: double, i94mon: double, i94cit: double, i94res: double, i94port: string, arrdate: double, i94mode: double, i94addr: string, depdate: double, i94bir: double, i94visa: double, count: double, dtadfile: string, visapost: string, occup: string, entdepa: string, entdepd: string, entdepu: string, matflag: string, biryear: double, dtaddto: string, gender: string, insnum: string, airline: string, admnum: double, fltno: string, visatype: string]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1786071"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "I.e., it seems like there weren't any duplicates left."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df = df.withColumn('immigration_id', expr('cast(cicid as int)')).withColumn('origin_id', expr('cast(i94cit as int)')).withColumn('visa_id', expr('cast(i94visa as int)')).withColumn('mode_id', expr('cast(i94mode as int)'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "get_timestamp_from_sas = udf(lambda sas: datetime.datetime(1960, 1, 1) + datetime.timedelta(days=int(sas)), TimestampType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df = df.withColumn('arrival_date', get_timestamp_from_sas(df.arrdate)).withColumn('departure_date', get_timestamp_from_sas(df.depdate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.printSchema of DataFrame[cicid: double, i94yr: double, i94mon: double, i94cit: double, i94res: double, i94port: string, arrdate: double, i94mode: double, i94addr: string, depdate: double, i94bir: double, i94visa: double, count: double, dtadfile: string, visapost: string, occup: string, entdepa: string, entdepd: string, entdepu: string, matflag: string, biryear: double, dtaddto: string, gender: string, insnum: string, airline: string, admnum: double, fltno: string, visatype: string, immigration_id: int, origin_id: int, visa_id: int, mode_id: int, arrival_date: timestamp, departure_date: timestamp]>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df = df.selectExpr('immigration_id', 'origin_id', 'i94addr as destination', \n",
    "                                                 'arrival_date', 'departure_date', 'dayofmonth(arrival_date) as day', 'month(arrival_date) as month', 'year(arrival_date) as year',\n",
    "                                                 'mode_id', 'visa_id', 'visatype as visa_type_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(immigration_id=9, origin_id=101, destination='CT', arrival_date=datetime.datetime(2016, 1, 16, 0, 0), departure_date=datetime.datetime(2016, 1, 27, 0, 0), day=16, month=1, year=2016, mode_id=1, visa_id=2, visa_type_id='B2')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Better use show() when using PySpark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------+-----------+-------------------+-------------------+---+-----+----+-------+-------+------------+\n",
      "|immigration_id|origin_id|destination|       arrival_date|     departure_date|day|month|year|mode_id|visa_id|visa_type_id|\n",
      "+--------------+---------+-----------+-------------------+-------------------+---+-----+----+-------+-------+------------+\n",
      "|             9|      101|         CT|2016-01-16 00:00:00|2016-01-27 00:00:00| 16|    1|2016|      1|      2|          B2|\n",
      "|            10|      101|         CT|2016-01-16 00:00:00|2016-02-15 00:00:00| 16|    1|2016|      1|      2|          B2|\n",
      "|            11|      101|         CT|2016-01-16 00:00:00|2016-02-15 00:00:00| 16|    1|2016|      1|      2|          B2|\n",
      "|            15|      101|         MA|2016-01-24 00:00:00|2016-03-11 00:00:00| 24|    1|2016|      1|      3|          F1|\n",
      "|            20|      101|         IL|2016-01-20 00:00:00|2016-01-29 00:00:00| 20|    1|2016|      1|      2|          B2|\n",
      "|            21|      101|         TN|2016-01-25 00:00:00|2016-02-11 00:00:00| 25|    1|2016|      1|      2|          B2|\n",
      "|            24|      101|         MI|2016-01-23 00:00:00|2016-02-24 00:00:00| 23|    1|2016|      1|      2|          B2|\n",
      "|            25|      101|         MI|2016-01-23 00:00:00|2016-02-24 00:00:00| 23|    1|2016|      1|      2|          B2|\n",
      "|            32|      101|         FL|2016-01-18 00:00:00|2016-02-17 00:00:00| 18|    1|2016|      1|      2|          B2|\n",
      "|            36|      101|         TN|2016-01-13 00:00:00|2016-01-26 00:00:00| 13|    1|2016|      1|      2|          B2|\n",
      "|            37|      101|         TN|2016-01-13 00:00:00|2016-01-26 00:00:00| 13|    1|2016|      1|      2|          B2|\n",
      "|            40|      101|         NY|2016-01-21 00:00:00|2016-01-28 00:00:00| 21|    1|2016|      1|      2|          B2|\n",
      "|            41|      101|         NY|2016-01-21 00:00:00|2016-02-07 00:00:00| 21|    1|2016|      1|      2|          B2|\n",
      "|            42|      101|         NY|2016-01-21 00:00:00|2016-02-21 00:00:00| 21|    1|2016|      1|      2|          B2|\n",
      "|            46|      101|         NY|2016-01-29 00:00:00|2016-02-07 00:00:00| 29|    1|2016|      1|      1|          B1|\n",
      "|            47|      101|         NY|2016-01-29 00:00:00|2016-02-07 00:00:00| 29|    1|2016|      1|      1|          B1|\n",
      "|            58|      101|         NY|2016-01-13 00:00:00|2016-03-28 00:00:00| 13|    1|2016|      1|      2|          B2|\n",
      "|            60|      101|         NJ|2016-01-15 00:00:00|2016-03-09 00:00:00| 15|    1|2016|      1|      3|          F1|\n",
      "|            62|      101|         NY|2016-01-16 00:00:00|2016-03-30 00:00:00| 16|    1|2016|      1|      2|          B2|\n",
      "|            63|      101|         NY|2016-01-19 00:00:00|2016-02-01 00:00:00| 19|    1|2016|      1|      2|          B2|\n",
      "+--------------+---------+-----------+-------------------+-------------------+---+-----+----+-------+-------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "*Map out the conceptual data model and explain why you chose that model*\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "*List the steps necessary to pipeline the data into the chosen data model*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "We select a star schema with a fact table in the middle, and four supplemental dimensional tables. The fact table will provide us with sufficiently fast OLAP performance while the much smaller dimensional tables contain additional information.\n",
    "\n",
    "The ETL-pipeline will not be too complicated, as the immigration dataset already contains all the information required for the fact table. As a result, no JOIN-operations are required, and the pipeline mostly consists of ignoring certain columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "<img src='images/ER_diagram.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "*Build the data pipelines to create the data model.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## 4.1.1 Fact Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "As already mentioned, the ETL pipeline for the fact table is rather simple to realize, i.e. JOINS are not needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def load_data(file):\n",
    "    \"\"\"\n",
    "    Description: Load the data\n",
    "    Arguments:   file: Filename of sas7bdat-file\n",
    "    Returns:     None\n",
    "    \"\"\"\n",
    "    file = '../../data/18-83510-I94-Data-2016/{}'.format(file)\n",
    "    df = spark.read.format('com.github.saurfang.sas.spark').load(file)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def clean_data(df):\n",
    "    \"\"\"\n",
    "    Description: Clean the data\n",
    "    Arguments:   df: (Spark-)dataframe\n",
    "    Returns:     Processed (Spark-)dataframe\n",
    "    \"\"\"\n",
    "    return df.filter(\"i94cit == i94res and i94addr != '' and visatype != '' and arrdate is not null and depdate is not null\").dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_ids_columns(df):\n",
    "    \"\"\"\n",
    "    Description: ds\n",
    "    Arguments: (Spark-)dataframe\n",
    "    Returns:   Processed (Spark-)dataframe\n",
    "    \"\"\"\n",
    "    return df.withColumn('immigration_id', expr('cast(cicid as int)')).withColumn('origin_id', expr('cast(i94cit as int)')).withColumn('visa_id', expr('cast(i94visa as int)')).withColumn('mode_id', expr('cast(i94mode as int)'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "get_timestamp_from_sas = udf(lambda sas: datetime.datetime(1960, 1, 1) + datetime.timedelta(days=int(sas)), TimestampType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_column_dates_from_sas_to_timestamp(df):\n",
    "    \"\"\"\n",
    "    Description: asd\n",
    "    Arguments:   df: (Spark-)dataframe\n",
    "    Returns:     df: Processed (Spark-)dataframe\n",
    "    \"\"\"\n",
    "    return df.withColumn('arrival_date', get_timestamp_from_sas(df.arrdate)).withColumn('departure_date', get_timestamp_from_sas(df.depdate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def select_columns(df):\n",
    "    \"\"\"\n",
    "    Description: asd\n",
    "    Arguments:   df: (Spark-)dataframe\n",
    "    Returns:     df: Processed (Spark-)dataframe\n",
    "    \"\"\"\n",
    "    return df.selectExpr('immigration_id', 'origin_id', 'i94addr as destination', 'arrival_date', 'departure_date', 'mode_id', 'visa_id', 'visatype as visa_type_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def etl_pipeline(batch):\n",
    "    \"\"\"\n",
    "    Description: The ETL-Pipeline. Processes a batch of sas-files.\n",
    "    Arguments:   batch: List of parquet-filenames\n",
    "    Returns:     None\n",
    "    \"\"\"\n",
    "    for num, name in enumerate(batch, start=1):\n",
    "        df = load_data(name)\n",
    "\n",
    "        df = clean_data(df)\n",
    "        df = get_ids_columns(df)\n",
    "        df = get_column_dates_from_sas_to_timestamp(df)\n",
    "        df = select_columns(df)\n",
    "\n",
    "        df.write.partitionBy('visa_type_id').parquet('data_parquet/{}/'.format(num), mode='overwrite')\n",
    "\n",
    "        print(\"Processed file {}: {}\".format(num, name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# We compile the file batch manually, as there are only twelve months to consider:\n",
    "batch = [\n",
    "    'i94_jan16_sub.sas7bdat',\n",
    "    'i94_feb16_sub.sas7bdat',\n",
    "    'i94_mar16_sub.sas7bdat',\n",
    "    'i94_apr16_sub.sas7bdat',\n",
    "    'i94_may16_sub.sas7bdat',\n",
    "    'i94_jun16_sub.sas7bdat',\n",
    "    'i94_jul16_sub.sas7bdat',\n",
    "    'i94_aug16_sub.sas7bdat',\n",
    "    'i94_sep16_sub.sas7bdat',\n",
    "    'i94_oct16_sub.sas7bdat',\n",
    "    'i94_nov16_sub.sas7bdat',\n",
    "    'i94_dec16_sub.sas7bdat'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Having prepared the pipeline above, we are set and ready to trigger the ETL-pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed file 1: i94_jan16_sub.sas7bdat\n",
      "Processed file 2: i94_feb16_sub.sas7bdat\n",
      "Processed file 3: i94_mar16_sub.sas7bdat\n",
      "Processed file 4: i94_apr16_sub.sas7bdat\n",
      "Processed file 5: i94_may16_sub.sas7bdat\n",
      "Processed file 6: i94_jun16_sub.sas7bdat\n",
      "Processed file 7: i94_jul16_sub.sas7bdat\n",
      "Processed file 8: i94_aug16_sub.sas7bdat\n",
      "Processed file 9: i94_sep16_sub.sas7bdat\n",
      "Processed file 10: i94_oct16_sub.sas7bdat\n",
      "Processed file 11: i94_nov16_sub.sas7bdat\n",
      "Processed file 12: i94_dec16_sub.sas7bdat\n"
     ]
    }
   ],
   "source": [
    "etl_pipeline(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "*Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:*\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness\n",
    " \n",
    "*Run Quality Checks*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------+-----------+-------------------+-------------------+-------+-------+------------+\n",
      "|immigration_id|origin_id|destination|       arrival_date|     departure_date|mode_id|visa_id|visa_type_id|\n",
      "+--------------+---------+-----------+-------------------+-------------------+-------+-------+------------+\n",
      "|          1153|      107|         IL|2016-01-16 00:00:00|2016-01-30 00:00:00|      1|      2|          B2|\n",
      "|         16124|      131|         NY|2016-01-08 00:00:00|2016-01-12 00:00:00|      1|      2|          B2|\n",
      "|         26940|      165|         CA|2016-01-01 00:00:00|2016-03-21 00:00:00|      1|      2|          B2|\n",
      "|         28627|      209|         GU|2016-01-06 00:00:00|2016-01-09 00:00:00|      1|      2|          B2|\n",
      "|         46913|      245|         IL|2016-01-08 00:00:00|2016-01-31 00:00:00|      1|      2|          B2|\n",
      "|         47174|      245|         OH|2016-01-14 00:00:00|2016-04-14 00:00:00|      1|      2|          B2|\n",
      "|         47442|      245|         WI|2016-01-21 00:00:00|2016-03-18 00:00:00|      1|      2|          B2|\n",
      "|         47629|      245|         CA|2016-01-29 00:00:00|2016-02-15 00:00:00|      1|      2|          B2|\n",
      "|         47714|      245|         TX|2016-01-22 00:00:00|2016-02-07 00:00:00|      1|      2|          B2|\n",
      "|         52709|      245|         CA|2016-01-23 00:00:00|2016-02-12 00:00:00|      1|      2|          B2|\n",
      "|         62044|      245|         CA|2016-01-17 00:00:00|2016-01-22 00:00:00|      1|      2|          B2|\n",
      "|         62872|      250|         CA|2016-01-17 00:00:00|2016-01-26 00:00:00|      1|      2|          B2|\n",
      "|         63171|      251|         NV|2016-01-20 00:00:00|2016-02-09 00:00:00|      1|      2|          B2|\n",
      "|         63904|      253|         II|2016-01-23 00:00:00|2016-03-27 00:00:00|      1|      2|          B2|\n",
      "|         70114|      258|         IL|2016-01-14 00:00:00|2016-02-09 00:00:00|      1|      2|          B2|\n",
      "|         70706|      258|         TX|2016-01-21 00:00:00|2016-02-15 00:00:00|      1|      2|          B2|\n",
      "|         71609|      258|         NJ|2016-01-06 00:00:00|2016-01-07 00:00:00|      1|      2|          B2|\n",
      "|         78175|      261|         FL|2016-01-11 00:00:00|2016-01-21 00:00:00|      1|      2|          B2|\n",
      "|         78447|      261|         MD|2016-01-04 00:00:00|2016-01-18 00:00:00|      1|      2|          B2|\n",
      "|         82564|      263|         NV|2016-01-18 00:00:00|2016-01-23 00:00:00|      1|      2|          B2|\n",
      "+--------------+---------+-----------+-------------------+-------------------+-------+-------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Perform quality checks here\n",
    "\n",
    "january_df = spark.read.parquet('data_parquet/1/')\n",
    "january_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "checks = [\n",
    "    'SELECT * FROM immigrations WHERE (immigration_id IS NULL OR immigration_id = \"\")',\n",
    "    'SELECT * FROM immigrations WHERE (origin_id IS NULL OR origin_id = \"\")',\n",
    "    'SELECT * FROM immigrations WHERE (mode_id IS NULL OR mode_id = \"\")',\n",
    "    'SELECT * FROM immigrations WHERE (visa_id IS NULL OR visa_id = \"\")',\n",
    "    'SELECT * FROM immigrations WHERE (visa_type_id IS NULL OR visa_type_id = \"\")'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def data_quality_checks(df, tests):\n",
    "    \"\"\"\n",
    "    Description: Runs the quality checks defined as a list of SQL expressions on a (Spark-)dataframe.\n",
    "    Arguments:   df:    (Spark-)dataframe to be tested\n",
    "                 tests: List of tests defined as SQL-expressions\n",
    "    Returns:     None\n",
    "    \"\"\"\n",
    "    df.createOrReplaceTempView('immigrations')\n",
    "    for num, test in enumerate(tests):\n",
    "        print(f\"Test {num}:\")\n",
    "        spark.sql(test).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 0:\n",
      "+--------------+---------+-----------+------------+--------------+---+-----+----+-------+-------+------------+\n",
      "|immigration_id|origin_id|destination|arrival_date|departure_date|day|month|year|mode_id|visa_id|visa_type_id|\n",
      "+--------------+---------+-----------+------------+--------------+---+-----+----+-------+-------+------------+\n",
      "+--------------+---------+-----------+------------+--------------+---+-----+----+-------+-------+------------+\n",
      "\n",
      "Test 1:\n",
      "+--------------+---------+-----------+------------+--------------+---+-----+----+-------+-------+------------+\n",
      "|immigration_id|origin_id|destination|arrival_date|departure_date|day|month|year|mode_id|visa_id|visa_type_id|\n",
      "+--------------+---------+-----------+------------+--------------+---+-----+----+-------+-------+------------+\n",
      "+--------------+---------+-----------+------------+--------------+---+-----+----+-------+-------+------------+\n",
      "\n",
      "Test 2:\n",
      "+--------------+---------+-----------+------------+--------------+---+-----+----+-------+-------+------------+\n",
      "|immigration_id|origin_id|destination|arrival_date|departure_date|day|month|year|mode_id|visa_id|visa_type_id|\n",
      "+--------------+---------+-----------+------------+--------------+---+-----+----+-------+-------+------------+\n",
      "+--------------+---------+-----------+------------+--------------+---+-----+----+-------+-------+------------+\n",
      "\n",
      "Test 3:\n",
      "+--------------+---------+-----------+------------+--------------+---+-----+----+-------+-------+------------+\n",
      "|immigration_id|origin_id|destination|arrival_date|departure_date|day|month|year|mode_id|visa_id|visa_type_id|\n",
      "+--------------+---------+-----------+------------+--------------+---+-----+----+-------+-------+------------+\n",
      "+--------------+---------+-----------+------------+--------------+---+-----+----+-------+-------+------------+\n",
      "\n",
      "Test 4:\n",
      "+--------------+---------+-----------+------------+--------------+---+-----+----+-------+-------+------------+\n",
      "|immigration_id|origin_id|destination|arrival_date|departure_date|day|month|year|mode_id|visa_id|visa_type_id|\n",
      "+--------------+---------+-----------+------------+--------------+---+-----+----+-------+-------+------------+\n",
      "+--------------+---------+-----------+------------+--------------+---+-----+----+-------+-------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_quality_checks(df, checks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data dictionary \n",
    "*Create a data dictionary for your data model. For each field, provide a brief description of what the data is and where it came from. You can include the data dictionary in the notebook or in a separate file.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "The data dictionary of the database is as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Fact Table: `immigrations`\n",
    "\n",
    "| Attribute | Description |\n",
    "|-------|-------|\n",
    "|immigration_id | unique identifier of immigration |\n",
    "|origin_id | this value represents the location where the person came and is the same value as i94cit and i94res columns  |\n",
    "|destination | state where the immigrant arrive (i94addr column) |\n",
    "|arrival_date| data that the person arrived in the us (datetime based in column arrdate) |\n",
    "|departure_time| data that the person left in the us (datetime based in column dapdate) |\n",
    "|mode_id| vehicle used to enter the country and this value is from i94mode |\n",
    "|visa_id| type of immigrant is the same value as i94visa column |\n",
    "|visa_type_id| visa type is the same value as visatype column |\n",
    "\n",
    "\n",
    "#### Dim Table: `country_of_origin`\n",
    "\n",
    "| Attribute | Description |\n",
    "|-------|-------|\n",
    "|origin_id| valid and invalid codes which indicates the location where the person came from |\n",
    "|description| location name |\n",
    "\n",
    "\n",
    "#### Dim Table: `visa`\n",
    "\n",
    "| Attribute | Description |\n",
    "|-------|-------|\n",
    "|visa_id| value which indicate the type of immigrant |\n",
    "|sescription| type name |\n",
    "\n",
    "#### Dim Table: `immigration_modes`\n",
    "\n",
    "| Attribute | Description |\n",
    "|-------|-------|\n",
    "|mode_id| value which represent the vehicle used to enter the country |\n",
    "|type| type of vehicle |\n",
    "\n",
    "#### Dim Table: `visa_types`\n",
    "\n",
    "| Attribute | Description |\n",
    "|-------|-------|\n",
    "|visa_type_id| code which indicate the visa type |\n",
    "|description| description about who hava access to this visa |\n",
    "|class| visa subclass |\n",
    "|subclass| visa class |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "* Propose how often the data should be updated and why.\n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.\n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    " * The database needed to be accessed by 100+ people."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Although we have used Parquet for the analytical data in the project, the main target should be a scalable warehouse solution like _AWS Redshift_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "In a realistic scenario, the provided data will be updated on a regular basis in a data lake. In order to make the most recent data available for fast analytical querying (e.g. for administrative or policy needs), the ETL process needs to update the database on a regular basis, for example on a monthly, weekly, or daily basis. One tool to realise this is _Apache Airflow_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "As for the update intervals, we propose a nightly update. _Airflow_ (or alternatively _AWS Step Functions_) can be configured such that the import data would be available early in the morning. Assuming that daily data will be available in the data lake around midnight, the ETL process can be automatically started in the middle of the night."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "It has to be determined how long an incremental update takes on the given system (e.g. 2 or 3 hours), such that the start of this batch job can be set and the data can be available before office hours in the morning. In order to increase the data import throughput, an idea is to chunk the data to parallelize the input (and the number of nodes increased accordingly)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Access to a higher userbase (e.g. more than 100 analysts) would require to scale the database appropriately to handle all incoming OLAP-requests (all read-only operations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
